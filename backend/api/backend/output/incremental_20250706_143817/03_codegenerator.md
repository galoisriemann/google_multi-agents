# CodeGenerator Output
**Agent**: CodeGenerator
**Execution Order**: 3
**Timestamp**: 2025-07-06 14:40:59

---

## Code Implementation

### Project Structure
```
project/
├── src/
│   ├── __init__.py
│   ├── main.py
│   ├── services/
│   │   ├── __init__.py
│   │   ├── data_ingestion.py
│   │   ├── market_data_processing.py
│   │   ├── llm_inference.py
│   │   ├── analytics_insights.py
│   │   ├── report_generation.py
│   │   └── continuous_monitoring.py
│   ├── models/
│   │   ├── __init__.py
│   │   └── report_models.py
│   ├── utils/
│   │   ├── __init__.py
│   │   ├── llm_utils.py
│   │   ├── data_utils.py
│   │   └── logger.py
│   └── config.py
└── tests/
    ├── __init__.py
    ├── test_data_ingestion.py
    ├── test_market_data_processing.py
    ├── test_llm_inference.py
    ├── test_analytics_insights.py
    ├── test_report_generation.py
    └── test_main.py
```

### Supporting Modules

```python
# src/config.py
"""
Configuration settings for the Market Research Report Generating Framework.
"""
import os

class Config:
    """
    General configuration settings for the framework.
    """
    LLM_API_KEY: str = os.getenv("LLM_API_KEY", "YOUR_LLM_API_KEY")
    LLM_MODEL_NAME: str = os.getenv("LLM_MODEL_NAME", "gpt-4")
    DATA_LAKE_PATH: str = "data/raw/"
    DATA_WAREHOUSE_PATH: str = "data/processed/"
    REPORTS_PATH: str = "reports/"
    LOG_LEVEL: str = os.getenv("LOG_LEVEL", "INFO").upper()

    # Define various data sources (conceptual)
    DATA_SOURCES: dict = {
        "news_api": {"url": "https://api.example.com/news", "key": "NEWS_API_KEY"},
        "sec_filings_db": {"url": "https://api.example.com/sec", "key": "SEC_API_KEY"},
        "social_media_stream": {"url": "https://api.example.com/social", "key": "SOCIAL_API_KEY"},
        "market_research_db": {"url": "https://api.example.com/market", "key": "MARKET_API_KEY"},
    }

# src/utils/logger.py
"""
Centralized logging utility for the Market Research Report Generating Framework.
"""
import logging
from src.config import Config

def get_logger(name: str) -> logging.Logger:
    """
    Configures and returns a logger instance.

    Args:
        name: The name of the logger, typically the module name (e.g., __name__).

    Returns:
        A configured logging.Logger instance.
    """
    logger = logging.getLogger(name)
    if not logger.handlers:
        logger.setLevel(Config.LOG_LEVEL)
        # Create console handler and set level
        ch = logging.StreamHandler()
        ch.setLevel(Config.LOG_LEVEL)
        # Create formatter
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        # Add formatter to ch
        ch.setFormatter(formatter)
        # Add ch to logger
        logger.addHandler(ch)
    return logger

# src/models/report_models.py
"""
Pydantic models for defining the data structures used throughout the framework.
"""
from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional

class RawMarketData(BaseModel):
    """Represents raw data ingested from various sources."""
    source: str = Field(..., description="The source of the raw data (e.g., 'news_api', 'sec_filings').")
    timestamp: str = Field(..., description="Timestamp of when the data was ingested (ISO format).")
    content: Dict[str, Any] = Field(..., description="The raw content from the source.")
    data_id: str = Field(..., description="Unique identifier for the raw data entry.")

class ProcessedMarketData(BaseModel):
    """Represents data after initial processing and structuring."""
    original_id: str = Field(..., description="ID of the raw data it originated from.")
    industry_sector: Optional[str] = Field(None, description="Identified industry sector.")
    companies: List[str] = Field([], description="List of companies mentioned.")
    keywords: List[str] = Field([], description="Extracted keywords.")
    summary: str = Field(..., description="A concise summary of the processed content.")
    sentiment: Optional[str] = Field(None, description="Overall sentiment (e.g., 'positive', 'negative', 'neutral').")
    processed_at: str = Field(..., description="Timestamp of when the data was processed.")
    structured_data: Dict[str, Any] = Field({}, description="Structured data derived from raw content.")

class LLMInsight(BaseModel):
    """Represents an insight generated by the LLM."""
    insight_type: str = Field(..., description="Type of insight (e.g., 'trend', 'competitive_advantage', 'prediction').")
    description: str = Field(..., description="Detailed description of the insight.")
    relevance_score: float = Field(..., description="Score indicating relevance (0-1).")
    confidence_score: float = Field(..., description="LLM's confidence in the insight (0-1).")
    supporting_data_ids: List[str] = Field([], description="List of processed data IDs supporting this insight.")
    generated_at: str = Field(..., description="Timestamp of insight generation.")

class Recommendation(BaseModel):
    """Represents an actionable recommendation."""
    category: str = Field(..., description="Category of the recommendation (e.g., 'Technology Adoption', 'Market Entry').")
    description: str = Field(..., description="Detailed description of the recommendation.")
    action_steps: List[str] = Field([], description="Concrete steps to implement the recommendation.")
    expected_impact: str = Field(..., description="Expected business impact of the recommendation.")
    priority: str = Field(..., description="Priority level (e.g., 'High', 'Medium', 'Low').")
    related_insights: List[str] = Field([], description="IDs or descriptions of related LLM insights.")

class ExecutiveSummary(BaseModel):
    """Represents the executive summary of the report."""
    key_findings: List[str] = Field(..., description="List of major findings.")
    strategic_implications: List[str] = Field(..., description="List of strategic implications.")
    top_recommendations: List[Recommendation] = Field(..., description="Top 3-5 key recommendations.")
    summary_text: str = Field(..., description="Full text of the executive summary.")

class ReportContent(BaseModel):
    """Structure for the complete Gartner-style report content."""
    executive_summary: ExecutiveSummary
    industry_analysis: Dict[str, Any] = Field(..., description="Industry overview, market size, growth drivers.")
    competitive_landscape: Dict[str, Any] = Field(..., description="Key players, market share, SWOT analysis.")
    market_trends_predictions: Dict[str, Any] = Field(..., description="Identified trends, future outlook, growth opportunities.")
    technology_adoption_analysis: Dict[str, Any] = Field(..., description="Relevant technologies, adoption rates, impact assessment.")
    strategic_insights: List[LLMInsight] = Field(..., description="Comprehensive list of generated strategic insights.")
    actionable_recommendations: List[Recommendation] = Field(..., description="Comprehensive list of actionable recommendations.")
    appendix: Optional[Dict[str, Any]] = Field(None, description="Supporting data, methodologies.")

class ReportRequest(BaseModel):
    """Model for a user's request to generate a report."""
    request_id: str = Field(..., description="Unique ID for the report request.")
    industry: str = Field(..., description="Target industry for the research.")
    focus_areas: List[str] = Field(..., description="Specific areas of focus (e.g., 'AI in Healthcare', 'EV Battery Tech').")
    competitors_of_interest: List[str] = Field([], description="Specific competitors to analyze.")
    report_format: str = Field("pdf", description="Desired output format (e.g., 'pdf', 'docx', 'html').")
    start_date: Optional[str] = Field(None, description="Start date for data collection (ISO format).")
    end_date: Optional[str] = Field(None, description="End date for data collection (ISO format).")

class ReportStatus(BaseModel):
    """Model for tracking the status of a report generation."""
    request_id: str
    status: str = Field(..., description="Current status of the report (e.g., 'PENDING', 'IN_PROGRESS', 'COMPLETED', 'FAILED').")
    progress: float = Field(0.0, description="Progress percentage (0-100).")
    last_updated: str = Field(..., description="Timestamp of the last status update.")
    report_path: Optional[str] = Field(None, description="Path to the generated report if completed.")
    error_message: Optional[str] = Field(None, description="Error message if generation failed.")


# src/utils/llm_utils.py
"""
Utility functions for interacting with Large Language Models (LLMs),
including prompt engineering and simulated responses.
"""
from typing import Dict, Any, List
from src.config import Config
from src.utils.logger import get_logger

logger = get_logger(__name__)

class LLMService:
    """
    A conceptual service for interacting with LLMs.
    In a real implementation, this would connect to an actual LLM API.
    """
    def __init__(self, api_key: str = Config.LLM_API_KEY, model_name: str = Config.LLM_MODEL_NAME):
        self.api_key = api_key
        self.model_name = model_name
        logger.info(f"Initialized LLMService with model: {self.model_name}")

    def generate_response(self, prompt: str, max_tokens: int = 500, temperature: float = 0.7) -> str:
        """
        Simulates an LLM API call to generate a text response.
        In a real scenario, this would use an SDK like openai.Completion.create.

        Args:
            prompt: The input prompt for the LLM.
            max_tokens: Maximum number of tokens in the generated response.
            temperature: Sampling temperature for creativity.

        Returns:
            A simulated LLM response string.
        """
        logger.debug(f"Simulating LLM response for prompt: {prompt[:100]}...")
        # Placeholder for actual LLM API call
        # Example of how to integrate a real LLM:
        # from openai import OpenAI
        # client = OpenAI(api_key=self.api_key)
        # response = client.chat.completions.create(
        #     model=self.model_name,
        #     messages=[{"role": "user", "content": prompt}],
        #     max_tokens=max_tokens,
        #     temperature=temperature,
        # )
        # return response.choices[0].message.content.strip()

        # Simple simulated responses based on prompt keywords
        if "industry analysis" in prompt.lower():
            return "Based on the provided data, the semiconductor industry is experiencing rapid growth driven by AI and IoT. Key players include NVIDIA, Intel, and AMD. Market size is projected to reach $X billion by 20XX."
        elif "market trends" in prompt.lower():
            return "Emerging trends include sustainable energy solutions, personalized healthcare, and advanced automation. Future predictions indicate a significant shift towards decentralized technologies."
        elif "technology adoption" in prompt.lower():
            return "AI adoption in enterprise software is at 40% and growing. Blockchain in supply chain management is still nascent but shows promise. Recommendations include investment in AI-powered analytics."
        elif "strategic insights" in prompt.lower():
            return "A key strategic insight is the increasing consumer demand for eco-friendly products, presenting an opportunity for green innovation. Another insight points to consolidation in the streaming market."
        elif "actionable recommendations" in prompt.lower():
            return "Recommendation 1: Diversify supply chain to reduce reliance on single regions. Recommendation 2: Invest in R&D for quantum computing applications. Recommendation 3: Form strategic partnerships for market expansion."
        elif "executive summary" in prompt.lower():
            return "This report highlights the transformative impact of AI on various sectors, identifies sustainable practices as a core market trend, and recommends strategic investments in emerging technologies to maintain competitive advantage. Key findings indicate robust growth in tech-driven markets."
        elif "competitive landscape" in prompt.lower():
            return "The competitive landscape is dominated by a few large entities with significant market share. New entrants are disrupting niche markets through innovation. SWOT analysis reveals strong R&D but vulnerability to regulatory changes for XYZ Corp."
        elif "summary and extract entities" in prompt.lower():
            return "Summary: This document discusses the Q3 earnings of TechCorp, showing a 15% revenue increase driven by cloud services. Entities: TechCorp, Q3, cloud services, revenue."
        else:
            return "Simulated LLM response: Data processing complete. Insight extraction is ongoing. Further analysis is required for comprehensive findings."

    def extract_entities_and_summary(self, text: str) -> Dict[str, Any]:
        """
        Uses LLM to extract key entities and provide a summary of the given text.

        Args:
            text: The input text to process.

        Returns:
            A dictionary containing 'summary' and 'entities'.
        """
        prompt = f"Summarize the following text and extract key entities (people, organizations, locations, products, events). Return as JSON:\n\nTEXT: {text}\n\nJSON:"
        # In a real scenario, you'd ensure the LLM outputs valid JSON
        simulated_response = self.generate_response(prompt, max_tokens=200)
        
        # Simple parsing for simulation; a real LLM would be prompted for strict JSON
        if "Summary:" in simulated_response and "Entities:" in simulated_response:
            summary_part = simulated_response.split("Summary:")[1].split("Entities:")[0].strip()
            entities_part = simulated_response.split("Entities:")[1].strip()
            entities = [e.strip() for e in entities_part.split(',') if e.strip()]
        else:
            summary_part = simulated_response
            entities = ["TechCorp", "InnovationLabs"] # Default simulated entities

        return {"summary": summary_part, "entities": entities}


# src/utils/data_utils.py
"""
Utility functions for data manipulation, transformation, and validation.
"""
from typing import Any, Dict
from datetime import datetime

def sanitize_data(data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Sanitizes input data by removing potentially harmful or irrelevant characters/fields.
    This is a placeholder for more robust data cleansing.

    Args:
        data: The input dictionary data.

    Returns:
        The sanitized dictionary data.
    """
    cleaned_data = data.copy()
    # Example: remove specific keys, strip strings
    for key, value in cleaned_data.items():
        if isinstance(value, str):
            cleaned_data[key] = value.strip()
    return cleaned_data

def convert_to_iso_timestamp(dt_obj: datetime) -> str:
    """
    Converts a datetime object to an ISO formatted string.

    Args:
        dt_obj: The datetime object.

    Returns:
        ISO formatted string.
    """
    return dt_obj.isoformat()

# src/services/data_ingestion.py
"""
Service responsible for collecting and pre-processing raw data from diverse external sources.
Acts as the AI agent for automated data aggregation.
"""
import uuid
from typing import Dict, Any, List
from datetime import datetime
from src.models.report_models import RawMarketData
from src.utils.logger import get_logger
from src.utils.data_utils import sanitize_data
from src.config import Config

logger = get_logger(__name__)

class DataIngestionService:
    """
    Manages data collection from various configured sources.
    """
    def __init__(self):
        self.data_sources = Config.DATA_SOURCES
        # Simulate a data lake storage (e.g., a list of raw data objects)
        self.data_lake: List[RawMarketData] = []
        logger.info("DataIngestionService initialized.")

    def _fetch_from_source(self, source_name: str, query: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Simulates fetching data from a specific external source.
        In a real system, this would involve API calls, web scraping, etc.

        Args:
            source_name: The name of the data source.
            query: Parameters for the data fetch (e.g., keywords, date range).

        Returns:
            A list of raw data dictionaries.
        """
        logger.info(f"Simulating data fetch from {source_name} with query: {query}")
        # Placeholder for actual data fetching logic
        if source_name == "news_api":
            return [
                {"title": "Tech Giant Q3 Earnings Up 15%", "text": "TechCorp reported a significant increase in revenue, primarily driven by its cloud services division. The CEO highlighted innovation as a key driver."},
                {"title": "Green Energy Investment Surges", "text": "New investments in renewable energy hit record highs, indicating a strong market trend towards sustainability. Solar and wind power lead the way."},
                {"title": "Competitor X Launches New Product", "text": "InnovationLabs unveiled its new AI-powered gadget, aiming to disrupt the smart home market. Analysts predict strong competition."}
            ]
        elif source_name == "sec_filings_db":
            return [
                {"company": "TechCorp", "filing_type": "10-K", "report_date": "2023-09-30", "content_summary": "Annual report detailing financial performance and strategic outlook."},
                {"company": "InnovationLabs", "filing_type": "8-K", "report_date": "2023-10-15", "content_summary": "Current report on recent product launch and market expansion plans."}
            ]
        # Add more simulated sources as needed
        return []

    def ingest_data(self, query: Dict[str, Any]) -> List[str]:
        """
        Aggregates data from all configured sources based on a given query.

        Args:
            query: A dictionary containing parameters like 'industry', 'keywords', 'start_date', 'end_date'.

        Returns:
            A list of IDs for the newly ingested raw data entries.
        """
        logger.info(f"Starting data ingestion for query: {query}")
        ingested_ids: List[str] = []
        for source_name in self.data_sources:
            try:
                raw_data_items = self._fetch_from_source(source_name, query)
                for item in raw_data_items:
                    cleaned_item = sanitize_data(item)
                    data_id = f"raw_{uuid.uuid4()}"
                    raw_data_entry = RawMarketData(
                        source=source_name,
                        timestamp=datetime.now().isoformat(),
                        content=cleaned_item,
                        data_id=data_id
                    )
                    self.data_lake.append(raw_data_entry) # Simulate storage
                    ingested_ids.append(data_id)
                    logger.debug(f"Ingested data_id: {data_id} from {source_name}")
            except Exception as e:
                logger.error(f"Error ingesting data from {source_name}: {e}")
        logger.info(f"Finished data ingestion. Total {len(ingested_ids)} new items.")
        return ingested_ids

    def get_raw_data(self, data_ids: List[str]) -> List[RawMarketData]:
        """
        Retrieves raw data entries from the data lake based on their IDs.

        Args:
            data_ids: A list of IDs of the raw data entries.

        Returns:
            A list of RawMarketData objects.
        """
        return [data for data in self.data_lake if data.data_id in data_ids]

# src/services/market_data_processing.py
"""
Service responsible for transforming raw data into a structured format suitable for LLM consumption
and further analysis, and loading it into a data warehouse.
"""
from typing import List, Dict, Any
from datetime import datetime
from src.models.report_models import RawMarketData, ProcessedMarketData
from src.utils.logger import get_logger
from src.utils.llm_utils import LLMService

logger = get_logger(__name__)

class MarketDataProcessingService:
    """
    Processes raw market data, extracts entities, summarizes content,
    and loads into a structured format.
    """
    def __init__(self, llm_service: LLMService):
        self.llm_service = llm_service
        # Simulate a data warehouse storage
        self.data_warehouse: List[ProcessedMarketData] = []
        logger.info("MarketDataProcessingService initialized.")

    def process_raw_data(self, raw_data_items: List[RawMarketData]) -> List[str]:
        """
        Transforms raw data items into structured ProcessedMarketData using LLM capabilities.

        Args:
            raw_data_items: A list of RawMarketData objects.

        Returns:
            A list of IDs for the newly processed data entries.
        """
        logger.info(f"Starting processing of {len(raw_data_items)} raw data items.")
        processed_ids: List[str] = []
        for raw_item in raw_data_items:
            try:
                content_text = str(raw_item.content) # Convert dict to string for LLM
                
                # Use LLM to extract summary and entities
                llm_output = self.llm_service.extract_entities_and_summary(content_text)
                
                summary = llm_output.get("summary", "No summary extracted.")
                entities = llm_output.get("entities", [])
                
                # Simulate further structuring (e.g., sentiment, industry identification)
                industry_sector = None
                if "semiconductor" in summary.lower() or "nvidia" in str(entities).lower():
                    industry_sector = "Semiconductor"
                elif "energy" in summary.lower() or "solar" in summary.lower():
                    industry_sector = "Renewable Energy"

                sentiment = "neutral" # Placeholder for actual sentiment analysis
                if "increase" in summary.lower() or "growth" in summary.lower():
                    sentiment = "positive"
                elif "disrupt" in summary.lower() or "competition" in summary.lower():
                    sentiment = "mixed"

                processed_data = ProcessedMarketData(
                    original_id=raw_item.data_id,
                    industry_sector=industry_sector,
                    companies=[e for e in entities if "Corp" in e or "Labs" in e],
                    keywords=[k for k in entities if k not in ["Corp", "Labs"]],
                    summary=summary,
                    sentiment=sentiment,
                    processed_at=datetime.now().isoformat(),
                    structured_data={
                        "source_type": raw_item.source,
                        "original_timestamp": raw_item.timestamp,
                        "raw_content_preview": content_text[:100] + "..."
                    }
                )
                self.data_warehouse.append(processed_data) # Simulate storage
                processed_ids.append(processed_data.original_id)
                logger.debug(f"Processed raw_data_id: {raw_item.data_id}")
            except Exception as e:
                logger.error(f"Error processing raw data ID {raw_item.data_id}: {e}")
        logger.info(f"Finished data processing. Total {len(processed_ids)} items processed.")
        return processed_ids

    def get_processed_data(self, original_ids: List[str]) -> List[ProcessedMarketData]:
        """
        Retrieves processed data entries from the data warehouse based on their original raw data IDs.

        Args:
            original_ids: A list of original raw data IDs.

        Returns:
            A list of ProcessedMarketData objects.
        """
        return [data for data in self.data_warehouse if data.original_id in original_ids]


# src/services/llm_inference.py
"""
Core intelligence component for interacting with Large Language Models for advanced analysis,
insight extraction, and content generation.
"""
from typing import List, Dict, Any
from datetime import datetime
from src.models.report_models import ProcessedMarketData, LLMInsight
from src.utils.logger import get_logger
from src.utils.llm_utils import LLMService

logger = get_logger(__name__)

class LLMInferenceService:
    """
    Orchestrates LLM interactions to extract insights from processed market data.
    """
    def __init__(self, llm_service: LLMService):
        self.llm_service = llm_service
        self.insights_store: List[LLMInsight] = [] # Simulate an insights store
        logger.info("LLMInferenceService initialized.")

    def _build_context_from_data(self, processed_data: List[ProcessedMarketData]) -> str:
        """
        Aggregates processed data into a single string context for the LLM.
        In a real RAG system, this would involve semantic search in a vector DB.

        Args:
            processed_data: List of ProcessedMarketData objects.

        Returns:
            A concatenated string representing the context.
        """
        context = []
        for item in processed_data:
            context.append(f"Source: {item.structured_data.get('source_type', 'N/A')}")
            context.append(f"Summary: {item.summary}")
            if item.companies:
                context.append(f"Companies: {', '.join(item.companies)}")
            if item.keywords:
                context.append(f"Keywords: {', '.join(item.keywords)}")
            context.append("-" * 20)
        return "\n".join(context)

    def generate_industry_analysis(self, processed_data: List[ProcessedMarketData]) -> Dict[str, Any]:
        """
        Generates industry analysis using the LLM.

        Args:
            processed_data: Relevant processed market data.

        Returns:
            A dictionary containing industry analysis details.
        """
        context = self._build_context_from_data(processed_data)
        prompt = f"""
        Based on the following processed market data, provide a comprehensive industry analysis.
        Include market size, growth drivers, key segments, and major trends.
        
        DATA CONTEXT:
        {context}
        
        INDUSTRY ANALYSIS:
        """
        response = self.llm_service.generate_response(prompt, max_tokens=1000)
        # Parse LLM response into structured dictionary (conceptual)
        return {
            "overview": response,
            "market_size_estimation": "Not directly calculated, but LLM indicates significant growth.",
            "growth_drivers_llm": ["AI adoption", "Sustainability initiatives", "Digital transformation"],
            "key_segments_llm": ["Cloud Computing", "Renewable Energy", "Smart Devices"]
        }

    def generate_competitive_landscape(self, processed_data: List[ProcessedMarketData], competitors_of_interest: List[str]) -> Dict[str, Any]:
        """
        Generates competitive landscape mapping and SWOT analysis for specific competitors.

        Args:
            processed_data: Relevant processed market data.
            competitors_of_interest: List of competitor names to focus on.

        Returns:
            A dictionary containing competitive landscape details.
        """
        context = self._build_context_from_data(processed_data)
        competitors_str = ", ".join(competitors_of_interest) if competitors_of_interest else "key players"
        prompt = f"""
        Analyze the competitive landscape based on the following processed market data, focusing on {competitors_str}.
        Identify key players, their market positioning, strengths, weaknesses, opportunities, and threats (SWOT analysis).

        DATA CONTEXT:
        {context}

        COMPETITIVE LANDSCAPE AND SWOT:
        """
        response = self.llm_service.generate_response(prompt, max_tokens=1000)
        # Parse LLM response into structured dictionary (conceptual)
        return {
            "overview": response,
            "key_players_identified": competitors_of_interest if competitors_of_interest else ["TechCorp", "InnovationLabs"],
            "swot_analysis": {
                "TechCorp": {"Strengths": ["Strong cloud portfolio"], "Weaknesses": ["Reliance on specific markets"]},
                "InnovationLabs": {"Strengths": ["Innovative products"], "Weaknesses": ["Smaller market share"]}
            }
        }

    def identify_market_trends_and_predictions(self, processed_data: List[ProcessedMarketData]) -> Dict[str, Any]:
        """
        Identifies current and emerging market trends and generates future predictions.

        Args:
            processed_data: Relevant processed market data.

        Returns:
            A dictionary containing market trends and predictions.
        """
        context = self._build_context_from_data(processed_data)
        prompt = f"""
        Based on the following processed market data, identify current and emerging market trends.
        Provide future predictions for these trends over the next 3-5 years.

        DATA CONTEXT:
        {context}

        MARKET TRENDS AND FUTURE PREDICTIONS:
        """
        response = self.llm_service.generate_response(prompt, max_tokens=1000)
        # Parse LLM response into structured dictionary (conceptual)
        return {
            "identified_trends": ["Sustainability", "Hyper-personalization", "AI-driven Automation"],
            "future_predictions": response,
            "growth_opportunities_llm": ["Green Tech", "Personalized Healthcare platforms"]
        }

    def analyze_technology_adoption(self, processed_data: List[ProcessedMarketData]) -> Dict[str, Any]:
        """
        Assesses the adoption rates and impact of relevant technologies.

        Args:
            processed_data: Relevant processed market data.

        Returns:
            A dictionary containing technology adoption analysis.
        """
        context = self._build_context_from_data(processed_data)
        prompt = f"""
        Based on the following processed market data, analyze the adoption rates and impact of relevant technologies
        (e.g., AI, Blockchain, Cloud Computing, IoT) within the specified market.

        DATA CONTEXT:
        {context}

        TECHNOLOGY ADOPTION ANALYSIS:
        """
        response = self.llm_service.generate_response(prompt, max_tokens=800)
        # Parse LLM response into structured dictionary (conceptual)
        return {
            "overview": response,
            "technology_focus": ["AI", "Cloud Computing", "Blockchain"],
            "adoption_rates_llm": {"AI": "High and growing", "Blockchain": "Emerging"},
            "impact_assessment_llm": "AI significantly enhances operational efficiency."
        }

    def generate_strategic_insights(self, industry_analysis: Dict[str, Any], competitive_landscape: Dict[str, Any], market_trends: Dict[str, Any], tech_adoption: Dict[str, Any]) -> List[LLMInsight]:
        """
        Extracts actionable strategic insights by synthesizing various analysis outputs.

        Args:
            industry_analysis: Output from industry analysis.
            competitive_landscape: Output from competitive landscape analysis.
            market_trends: Output from market trends and predictions.
            tech_adoption: Output from technology adoption analysis.

        Returns:
            A list of LLMInsight objects.
        """
        combined_context = f"""
        Industry Analysis: {industry_analysis.get('overview', '')}
        Competitive Landscape: {competitive_landscape.get('overview', '')}
        Market Trends & Predictions: {market_trends.get('future_predictions', '')}
        Technology Adoption: {tech_adoption.get('overview', '')}
        """
        prompt = f"""
        Based on the following comprehensive analysis reports, extract key strategic insights.
        Each insight should be concise, data-driven, and highlight a significant implication.
        Provide each insight with a type (e.g., 'Market Opportunity', 'Competitive Threat', 'Technological Imperative').
        
        COMBINED ANALYSIS:
        {combined_context}
        
        STRATEGIC INSIGHTS:
        """
        response = self.llm_service.generate_response(prompt, max_tokens=1500)
        
        # Simulate parsing into LLMInsight objects
        insights: List[LLMInsight] = []
        # A more robust LLM would return structured JSON for direct parsing
        if "opportunity" in response.lower():
            insights.append(LLMInsight(
                insight_type="Market Opportunity",
                description="The surging demand for sustainable products presents a significant green innovation market opportunity.",
                relevance_score=0.9,
                confidence_score=0.85,
                supporting_data_ids=[], # In a real system, track actual data IDs
                generated_at=datetime.now().isoformat()
            ))
        if "threat" in response.lower() or "competition" in response.lower():
            insights.append(LLMInsight(
                insight_type="Competitive Threat",
                description="Aggressive entry of new AI-powered startups poses a competitive threat to established players in the smart home sector.",
                relevance_score=0.85,
                confidence_score=0.8,
                supporting_data_ids=[],
                generated_at=datetime.now().isoformat()
            ))
        if "investment" in response.lower() or "imperative" in response.lower():
            insights.append(LLMInsight(
                insight_type="Technological Imperative",
                description="Strategic investment in advanced AI analytics and blockchain for supply chain optimization is imperative for future growth and efficiency.",
                relevance_score=0.95,
                confidence_score=0.9,
                supporting_data_ids=[],
                generated_at=datetime.now().isoformat()
            ))
        
        return insights


# src/services/analytics_insights.py
"""
Applies structured analytical methods and statistical models to processed data and LLM outputs
to generate deep insights and validate LLM outputs. Also responsible for generating actionable recommendations.
"""
from typing import List, Dict, Any
from datetime import datetime
from src.models.report_models import LLMInsight, Recommendation
from src.utils.logger import get_logger
from src.utils.llm_utils import LLMService

logger = get_logger(__name__)

class AnalyticsInsightsService:
    """
    Performs structured analysis and generates actionable recommendations based on LLM insights.
    """
    def __init__(self, llm_service: LLMService):
        self.llm_service = llm_service
        logger.info("AnalyticsInsightsService initialized.")

    def validate_llm_insights(self, llm_insights: List[LLMInsight], processed_data: List[Any]) -> List[LLMInsight]:
        """
        Conceptually validates LLM-generated insights against raw/processed data.
        In a real system, this would involve cross-referencing facts, statistical checks.

        Args:
            llm_insights: List of LLMInsight objects.
            processed_data: Relevant processed data (for conceptual validation).

        Returns:
            A list of validated LLMInsight objects (or filtered/adjusted ones).
        """
        logger.info(f"Validating {len(llm_insights)} LLM insights.")
        validated_insights = []
        for insight in llm_insights:
            # Simple conceptual validation: check if keywords from insight appear in processed data summaries
            is_valid = False
            for data_item in processed_data:
                if isinstance(data_item, dict): # Handle dict if processed_data is not strictly ProcessedMarketData
                    summary_text = data_item.get("summary", "")
                else: # Assume ProcessedMarketData
                    summary_text = data_item.summary

                if any(keyword in summary_text.lower() for keyword in insight.description.lower().split()):
                    is_valid = True
                    break
            
            if is_valid and insight.confidence_score > 0.6: # Apply confidence threshold
                validated_insights.append(insight)
            else:
                logger.warning(f"Insight '{insight.description[:50]}...' failed validation or confidence check.")
        logger.info(f"Finished validation. {len(validated_insights)} insights passed.")
        return validated_insights

    def generate_actionable_recommendations(self, strategic_insights: List[LLMInsight], analysis_context: Dict[str, Any]) -> List[Recommendation]:
        """
        Translates strategic insights into concrete, practical, and actionable recommendations.

        Args:
            strategic_insights: List of LLMInsight objects.
            analysis_context: Additional context from other analysis components (e.g., industry, tech trends).

        Returns:
            A list of Recommendation objects.
        """
        logger.info(f"Generating recommendations from {len(strategic_insights)} strategic insights.")
        recommendations: List[Recommendation] = []
        
        insight_descriptions = "\n".join([f"- {i.description} (Type: {i.insight_type})" for i in strategic_insights])
        full_prompt = f"""
        Given the following strategic insights and market analysis context, generate concrete, actionable recommendations for businesses.
        Each recommendation should include specific action steps, expected impact, and a priority level (High, Medium, Low).
        
        Strategic Insights:
        {insight_descriptions}
        
        Market Analysis Context:
        Industry: {analysis_context.get('industry_name', 'General')}
        Key Market Trends: {', '.join(analysis_context.get('market_trends', []))}
        Technology Focus: {', '.join(analysis_context.get('tech_focus', []))}
        
        Actionable Recommendations:
        """
        llm_response = self.llm_service.generate_response(full_prompt, max_tokens=1000)
        
        # Simulate parsing LLM response into structured recommendations
        # A more advanced LLM integration would use function calling or strict JSON mode
        
        # Example 1: Based on Market Opportunity
        if any("market opportunity" in i.insight_type.lower() for i in strategic_insights):
            recommendations.append(Recommendation(
                category="Product Development & Innovation",
                description="Invest in R&D for sustainable product lines and eco-friendly manufacturing processes to capitalize on growing consumer demand.",
                action_steps=["Formulate dedicated green product teams.", "Allocate 15% of R&D budget to sustainable innovation.", "Seek eco-certifications."],
                expected_impact="Increased market share in the sustainable segment and enhanced brand reputation.",
                priority="High",
                related_insights=["Market Opportunity: Surging demand for sustainable products"]
            ))
        
        # Example 2: Based on Competitive Threat
        if any("competitive threat" in i.insight_type.lower() for i in strategic_insights):
            recommendations.append(Recommendation(
                category="Competitive Strategy",
                description="Develop a rapid response innovation cycle to counter new AI-powered market entrants by leveraging existing customer base and brand loyalty.",
                action_steps=["Monitor competitor product launches weekly.", "Launch beta programs for new features every quarter.", "Enhance customer engagement strategies."],
                expected_impact="Mitigation of market share loss and maintenance of competitive edge.",
                priority="Medium",
                related_insights=["Competitive Threat: Aggressive AI startup entry"]
            ))

        # Example 3: Based on Technological Imperative
        if any("technological imperative" in i.insight_type.lower() for i in strategic_insights):
            recommendations.append(Recommendation(
                category="Technology Adoption & Infrastructure",
                description="Implement advanced AI analytics solutions across operational departments to optimize efficiency and decision-making processes.",
                action_steps=["Conduct feasibility study for AI integration.", "Pilot AI tools in supply chain and customer service.", "Train workforce on new AI platforms."],
                expected_impact="Significant improvements in operational efficiency and data-driven strategic planning.",
                priority="High",
                related_insights=["Technological Imperative: Investment in AI analytics"]
            ))

        logger.info(f"Generated {len(recommendations)} actionable recommendations.")
        return recommendations


# src/services/report_generation.py
"""
Service responsible for compiling all processed data, LLM outputs, and analytical insights
into the final "Gartner-style" report format.
"""
from typing import Dict, Any, List
from datetime import datetime
import os
from src.models.report_models import (
    ReportContent, ExecutiveSummary, LLMInsight, Recommendation,
    ProcessedMarketData, ReportRequest
)
from src.utils.logger import get_logger
from src.utils.llm_utils import LLMService
from src.config import Config

logger = get_logger(__name__)

class ReportGenerationService:
    """
    Assembles all generated insights and data into a structured Gartner-style report.
    """
    def __init__(self, llm_service: LLMService):
        self.llm_service = llm_service
        self.reports_dir = Config.REPORTS_PATH
        os.makedirs(self.reports_dir, exist_ok=True)
        logger.info(f"ReportGenerationService initialized. Reports will be saved in: {self.reports_dir}")

    def _generate_executive_summary_llm(self, report_content: ReportContent) -> ExecutiveSummary:
        """
        Uses LLM to generate a concise executive summary based on the full report content.

        Args:
            report_content: The nearly complete report content object.

        Returns:
            An ExecutiveSummary object.
        """
        summary_prompt = f"""
        Generate a concise Executive Summary (approx. 200-300 words) for a Gartner-style market research report.
        Highlight the key findings, strategic implications, and the top 3-5 most important actionable recommendations.

        Report Key Sections:
        - Industry Analysis: {report_content.industry_analysis.get('overview', '')[:500]}...
        - Competitive Landscape: {report_content.competitive_landscape.get('overview', '')[:500]}...
        - Market Trends & Predictions: {report_content.market_trends_predictions.get('future_predictions', '')[:500]}...
        - Technology Adoption Analysis: {report_content.technology_adoption_analysis.get('overview', '')[:500]}...

        Strategic Insights:
        {'\n'.join([f"- {i.description}" for i in report_content.strategic_insights])}

        Top Actionable Recommendations:
        {'\n'.join([f"- {r.description} (Priority: {r.priority})" for r in report_content.actionable_recommendations[:5]])}

        EXECUTIVE SUMMARY:
        """
        
        summary_text = self.llm_service.generate_response(summary_prompt, max_tokens=400)
        
        # Extract key findings and strategic implications from the generated summary
        # This parsing is conceptual; a real LLM might be prompted for structured output.
        key_findings = ["Robust growth in cloud services.", "Surging demand for sustainable products.", "Increased competition from AI startups."]
        strategic_implications = ["Need for rapid innovation.", "Opportunity for green market leadership.", "Importance of AI adoption for efficiency."]
        
        # Select top recommendations for the summary (e.g., top 3 High priority)
        top_recommendations = sorted(report_content.actionable_recommendations, key=lambda x: {"High": 3, "Medium": 2, "Low": 1}.get(x.priority, 0), reverse=True)[:3]

        return ExecutiveSummary(
            key_findings=key_findings,
            strategic_implications=strategic_implications,
            top_recommendations=top_recommendations,
            summary_text=summary_text
        )

    def generate_report(
        self,
        request_id: str,
        industry_analysis: Dict[str, Any],
        competitive_landscape: Dict[str, Any],
        market_trends_predictions: Dict[str, Any],
        technology_adoption_analysis: Dict[str, Any],
        strategic_insights: List[LLMInsight],
        actionable_recommendations: List[Recommendation],
        report_request: ReportRequest
    ) -> str:
        """
        Assembles and generates the full Gartner-style market research report.

        Args:
            request_id: The ID of the report request.
            industry_analysis: Output from LLMInferenceService.
            competitive_landscape: Output from LLMInferenceService.
            market_trends_predictions: Output from LLMInferenceService.
            technology_adoption_analysis: Output from LLMInferenceService.
            strategic_insights: List of LLMInsight objects from LLMInferenceService/AnalyticsInsightsService.
            actionable_recommendations: List of Recommendation objects from AnalyticsInsightsService.
            report_request: The original report request object.

        Returns:
            The file path to the generated report.
        """
        logger.info(f"Starting report generation for request ID: {request_id}")

        report_content = ReportContent(
            executive_summary=ExecutiveSummary(
                key_findings=[], strategic_implications=[], top_recommendations=[], summary_text="" # Placeholder
            ),
            industry_analysis=industry_analysis,
            competitive_landscape=competitive_landscape,
            market_trends_predictions=market_trends_predictions,
            technology_adoption_analysis=technology_adoption_analysis,
            strategic_insights=strategic_insights,
            actionable_recommendations=actionable_recommendations,
            appendix={
                "data_sources_used": list(Config.DATA_SOURCES.keys()),
                "methodology_notes": "LLM-guided analysis with human validation principles."
            }
        )
        
        # Generate the Executive Summary after other sections are conceptually populated
        report_content.executive_summary = self._generate_executive_summary_llm(report_content)

        report_filename = f"market_research_report_{request_id}.{report_request.report_format}"
        report_filepath = os.path.join(self.reports_dir, report_filename)

        # Simulate writing the report to a file based on format
        with open(report_filepath, "w", encoding="utf-8") as f:
            f.write(f"# Gartner-Style Market Research Report\n\n")
            f.write(f"## Executive Summary\n\n")
            f.write(f"{report_content.executive_summary.summary_text}\n\n")
            f.write(f"### Key Findings\n")
            for finding in report_content.executive_summary.key_findings:
                f.write(f"- {finding}\n")
            f.write(f"\n### Strategic Implications\n")
            for implication in report_content.executive_summary.strategic_implications:
                f.write(f"- {implication}\n")
            f.write(f"\n### Top Recommendations\n")
            for rec in report_content.executive_summary.top_recommendations:
                f.write(f"- **{rec.description}** (Priority: {rec.priority})\n")
            
            f.write(f"\n## 1. Industry Analysis\n\n")
            f.write(f"{report_content.industry_analysis.get('overview', 'N/A')}\n\n")
            
            f.write(f"## 2. Competitive Landscape Mapping\n\n")
            f.write(f"{report_content.competitive_landscape.get('overview', 'N/A')}\n\n")
            
            f.write(f"## 3. Market Trends Identification and Future Predictions\n\n")
            f.write(f"{report_content.market_trends_predictions.get('future_predictions', 'N/A')}\n\n")
            
            f.write(f"## 4. Technology Adoption Analysis and Recommendations\n\n")
            f.write(f"{report_content.technology_adoption_analysis.get('overview', 'N/A')}\n\n")
            
            f.write(f"## 5. Strategic Insights\n\n")
            for insight in report_content.strategic_insights:
                f.write(f"### {insight.insight_type}\n")
                f.write(f"- {insight.description} (Relevance: {insight.relevance_score:.2f}, Confidence: {insight.confidence_score:.2f})\n\n")
            
            f.write(f"## 6. Actionable Recommendations\n\n")
            for rec in report_content.actionable_recommendations:
                f.write(f"### {rec.category} (Priority: {rec.priority})\n")
                f.write(f"- **Recommendation:** {rec.description}\n")
                f.write(f"- **Action Steps:** {'; '.join(rec.action_steps)}\n")
                f.write(f"- **Expected Impact:** {rec.expected_impact}\n\n")

            f.write(f"## Appendix\n\n")
            f.write(f"Data Sources: {', '.join(report_content.appendix.get('data_sources_used', []))}\n")
            f.write(f"Methodology: {report_content.appendix.get('methodology_notes', '')}\n")


        logger.info(f"Report for request ID {request_id} generated at: {report_filepath}")
        return report_filepath

# src/services/continuous_monitoring.py
"""
Orchestrates scheduled data ingestion, re-analysis, and report updates based on real-time market changes.
This service primarily listens for events or triggers based on schedules.
"""
from typing import Callable, Dict, Any
import time
from datetime import datetime, timedelta
from src.models.report_models import ReportRequest, ReportStatus
from src.utils.logger import get_logger

logger = get_logger(__name__)

class ContinuousMonitoringService:
    """
    Monitors market developments and triggers report updates.
    In a real system, this would be event-driven via a message broker
    or a dedicated scheduler (e.g., Airflow, Kubernetes CronJobs).
    """
    def __init__(self, trigger_report_generation_callback: Callable[[ReportRequest], ReportStatus]):
        self.trigger_report_generation = trigger_report_generation_callback
        self.monitored_requests: Dict[str, ReportRequest] = {}
        self.monitoring_interval_hours = 24 # Simulate daily monitoring
        logger.info("ContinuousMonitoringService initialized.")

    def register_for_monitoring(self, report_request: ReportRequest):
        """
        Registers a report request for continuous monitoring.

        Args:
            report_request: The ReportRequest object to monitor.
        """
        self.monitored_requests[report_request.request_id] = report_request
        logger.info(f"Report request {report_request.request_id} registered for continuous monitoring.")

    def _check_and_trigger_update(self, request_id: str, report_request: ReportRequest):
        """
        Conceptual check for triggering an update. In a real system, this would involve
        checking for new data ingested since the last report, or significant market shifts.

        Args:
            request_id: The ID of the report request.
            report_request: The ReportRequest object.
        """
        # Simulate conditions for update: e.g., if it's been more than X hours since last update
        # For simplicity, we'll just re-trigger for demonstration.
        
        # In a real scenario, compare current market data fingerprint/hash with previous one
        # or check for specific event types (e.g., 'major_acquisition_alert').

        logger.info(f"Checking for updates for report request {request_id}...")
        
        # For demonstration, always trigger if registered for monitoring
        logger.info(f"Triggering update for report request {request_id} due to simulated new data.")
        status = self.trigger_report_generation(report_request)
        logger.info(f"Update triggered for {request_id}, new status: {status.status}")

    def start_monitoring_loop(self, duration_seconds: int = 10, check_interval_seconds: int = 2):
        """
        Simulates a continuous monitoring loop. This would be a long-running process.

        Args:
            duration_seconds: How long the simulation should run.
            check_interval_seconds: How often to check for updates.
        """
        logger.info(f"Starting continuous monitoring loop for {duration_seconds} seconds...")
        end_time = datetime.now() + timedelta(seconds=duration_seconds)

        while datetime.now() < end_time:
            for request_id, report_request in list(self.monitored_requests.items()):
                self._check_and_trigger_update(request_id, report_request)
            time.sleep(check_interval_seconds) # Wait before next check
        logger.info("Continuous monitoring loop ended.")

```

### Main Implementation

```python
# src/main.py
"""
Orchestrates the entire LLM-guided Gartner-style market research report generation framework.
This acts as a high-level manager, coordinating calls between different microservices.
"""
import uuid
from datetime import datetime
from typing import Dict, Any, List

from src.models.report_models import ReportRequest, ReportStatus, LLMInsight, Recommendation
from src.services.data_ingestion import DataIngestionService
from src.services.market_data_processing import MarketDataProcessingService
from src.services.llm_inference import LLMInferenceService
from src.services.analytics_insights import AnalyticsInsightsService
from src.services.report_generation import ReportGenerationService
from src.services.continuous_monitoring import ContinuousMonitoringService
from src.utils.llm_utils import LLMService
from src.utils.logger import get_logger

logger = get_logger(__name__)

class MarketResearchFramework:
    """
    The main orchestrator for the Gartner-style market research report generation.
    Manages the flow from request to final report.
    """
    def __init__(self):
        # Initialize core LLM utility
        self.llm_service = LLMService()

        # Initialize all dependent services
        self.data_ingestion_service = DataIngestionService()
        self.market_data_processing_service = MarketDataProcessingService(self.llm_service)
        self.llm_inference_service = LLMInferenceService(self.llm_service)
        self.analytics_insights_service = AnalyticsInsightsService(self.llm_service)
        self.report_generation_service = ReportGenerationService(self.llm_service)
        
        # Continuous monitoring service needs a callback to trigger report generation
        self.continuous_monitoring_service = ContinuousMonitoringService(
            trigger_report_generation_callback=self.generate_market_research_report
        )

        self.report_statuses: Dict[str, ReportStatus] = {}
        logger.info("MarketResearchFramework initialized. All services are ready.")

    def generate_market_research_report(self, request: ReportRequest) -> ReportStatus:
        """
        Initiates the end-to-end process of generating a market research report.

        Args:
            request: A ReportRequest object detailing the research requirements.

        Returns:
            A ReportStatus object indicating the current status of the report generation.
        """
        report_status = ReportStatus(
            request_id=request.request_id,
            status="IN_PROGRESS",
            progress=0.0,
            last_updated=datetime.now().isoformat()
        )
        self.report_statuses[request.request_id] = report_status
        logger.info(f"Started report generation for request ID: {request.request_id}")

        try:
            # Stage 1: Data Ingestion
            logger.info("Stage 1: Data Ingestion")
            report_status.progress = 10.0
            raw_data_query = {
                "industry": request.industry,
                "focus_areas": request.focus_areas,
                "competitors": request.competitors_of_interest,
                "start_date": request.start_date,
                "end_date": request.end_date
            }
            raw_data_ids = self.data_ingestion_service.ingest_data(raw_data_query)
            if not raw_data_ids:
                raise ValueError("No raw data ingested for the given request criteria.")
            raw_data_items = self.data_ingestion_service.get_raw_data(raw_data_ids)
            report_status.last_updated = datetime.now().isoformat()

            # Stage 2: Market Data Processing
            logger.info("Stage 2: Market Data Processing")
            report_status.progress = 30.0
            processed_data_ids = self.market_data_processing_service.process_raw_data(raw_data_items)
            processed_data = self.market_data_processing_service.get_processed_data(processed_data_ids)
            if not processed_data:
                raise ValueError("No processed data available after processing.")
            report_status.last_updated = datetime.now().isoformat()

            # Stage 3: LLM-Guided Analysis (Core Insights Generation)
            logger.info("Stage 3: LLM-Guided Analysis")
            report_status.progress = 50.0
            
            industry_analysis = self.llm_inference_service.generate_industry_analysis(processed_data)
            competitive_landscape = self.llm_inference_service.generate_competitive_landscape(processed_data, request.competitors_of_interest)
            market_trends_predictions = self.llm_inference_service.identify_market_trends_and_predictions(processed_data)
            technology_adoption_analysis = self.llm_inference_service.analyze_technology_adoption(processed_data)
            
            strategic_insights = self.llm_inference_service.generate_strategic_insights(
                industry_analysis, competitive_landscape, market_trends_predictions, technology_adoption_analysis
            )
            report_status.last_updated = datetime.now().isoformat()

            # Stage 4: Analytics and Actionable Recommendations
            logger.info("Stage 4: Analytics and Actionable Recommendations")
            report_status.progress = 70.0
            validated_insights = self.analytics_insights_service.validate_llm_insights(strategic_insights, processed_data)
            
            analysis_context = {
                "industry_name": request.industry,
                "market_trends": market_trends_predictions.get("identified_trends", []),
                "tech_focus": technology_adoption_analysis.get("technology_focus", [])
            }
            actionable_recommendations = self.analytics_insights_service.generate_actionable_recommendations(
                validated_insights, analysis_context
            )
            report_status.last_updated = datetime.now().isoformat()

            # Stage 5: Report Generation
            logger.info("Stage 5: Report Generation")
            report_status.progress = 90.0
            final_report_path = self.report_generation_service.generate_report(
                request.request_id,
                industry_analysis,
                competitive_landscape,
                market_trends_predictions,
                technology_adoption_analysis,
                validated_insights,
                actionable_recommendations,
                request
            )
            report_status.report_path = final_report_path
            report_status.last_updated = datetime.now().isoformat()

            report_status.status = "COMPLETED"
            report_status.progress = 100.0
            logger.info(f"Report generation for request ID {request.request_id} completed successfully.")

        except Exception as e:
            logger.error(f"Error generating report for request ID {request.request_id}: {e}", exc_info=True)
            report_status.status = "FAILED"
            report_status.error_message = str(e)
            report_status.progress = 0.0 # Reset progress on failure
            report_status.last_updated = datetime.now().isoformat()

        return report_status

    def get_report_status(self, request_id: str) -> ReportStatus:
        """
        Retrieves the current status of a report generation request.

        Args:
            request_id: The ID of the report request.

        Returns:
            A ReportStatus object.
        """
        return self.report_statuses.get(request_id, ReportStatus(request_id=request_id, status="NOT_FOUND", progress=0.0, last_updated=datetime.now().isoformat()))

    def start_continuous_monitoring(self, duration_seconds: int = 30):
        """
        Starts the continuous monitoring loop for registered reports.

        Args:
            duration_seconds: How long the simulation should run.
        """
        self.continuous_monitoring_service.start_monitoring_loop(duration_seconds)

if __name__ == "__main__":
    framework = MarketResearchFramework()

    # Example Report Request 1
    request_id_1 = f"report_{uuid.uuid4()}"
    report_request_1 = ReportRequest(
        request_id=request_id_1,
        industry="Semiconductor Industry",
        focus_areas=["AI chips", "Market Consolidation"],
        competitors_of_interest=["NVIDIA", "Intel", "AMD"],
        report_format="md", # Using markdown for easy viewing
        start_date="2023-01-01",
        end_date="2023-12-31"
    )

    print(f"\n--- Initiating Report Generation for Request ID: {report_request_1.request_id} ---")
    status_1 = framework.generate_market_research_report(report_request_1)
    print(f"Report 1 Final Status: {status_1.status}, Path: {status_1.report_path}")
    print(f"Check the generated report at: {status_1.report_path}")

    # Example Report Request 2 (simulating a simpler request)
    request_id_2 = f"report_{uuid.uuid4()}"
    report_request_2 = ReportRequest(
        request_id=request_id_2,
        industry="Renewable Energy",
        focus_areas=["Solar Power Trends", "Investment Opportunities"],
        report_format="md"
    )

    print(f"\n--- Initiating Report Generation for Request ID: {report_request_2.request_id} ---")
    status_2 = framework.generate_market_research_report(report_request_2)
    print(f"Report 2 Final Status: {status_2.status}, Path: {status_2.report_path}")
    print(f"Check the generated report at: {status_2.report_path}")


    # Demonstrate continuous monitoring (will trigger re-generation of registered reports)
    print("\n--- Registering Report 1 for Continuous Monitoring ---")
    framework.continuous_monitoring_service.register_for_monitoring(report_request_1)
    
    # Run monitoring loop for a short duration to show it triggers
    print("\n--- Starting Continuous Monitoring Simulation (will re-trigger Report 1) ---")
    framework.start_continuous_monitoring(duration_seconds=10)

    print("\n--- Framework Execution Complete ---")

```

### Unit Tests

```python
# tests/__init__.py
# Empty file to mark the directory as a Python package

# tests/test_data_ingestion.py
import unittest
import os
import shutil
from datetime import datetime
from src.services.data_ingestion import DataIngestionService
from src.models.report_models import RawMarketData
from src.config import Config

class TestDataIngestionService(unittest.TestCase):
    def setUp(self):
        self.service = DataIngestionService()
        self.test_query = {
            "industry": "Test Industry",
            "keywords": ["test_keyword"],
            "start_date": "2023-01-01",
            "end_date": "2023-01-02"
        }
        # Clear the in-memory data lake for each test
        self.service.data_lake = []

    def test_ingest_data_success(self):
        """Test successful data ingestion from simulated sources."""
        ingested_ids = self.service.ingest_data(self.test_query)
        self.assertGreater(len(ingested_ids), 0)
        self.assertEqual(len(self.service.data_lake), len(ingested_ids))
        
        # Verify structure of ingested data
        for data_id in ingested_ids:
            raw_data = next((d for d in self.service.data_lake if d.data_id == data_id), None)
            self.assertIsNotNone(raw_data)
            self.assertIsInstance(raw_data, RawMarketData)
            self.assertIn(raw_data.source, Config.DATA_SOURCES.keys())
            self.assertIn("content", raw_data.content) # Check for basic content
            self.assertIsNotNone(raw_data.timestamp)

    def test_get_raw_data(self):
        """Test retrieving specific raw data entries."""
        ingested_ids = self.service.ingest_data(self.test_query)
        retrieved_data = self.service.get_raw_data([ingested_ids[0]])
        self.assertEqual(len(retrieved_data), 1)
        self.assertEqual(retrieved_data[0].data_id, ingested_ids[0])

    def test_ingest_data_no_sources(self):
        """Test ingestion when no data sources are configured (simulated)."""
        original_sources = self.service.data_sources
        self.service.data_sources = {} # Temporarily remove sources
        ingested_ids = self.service.ingest_data(self.test_query)
        self.assertEqual(len(ingested_ids), 0)
        self.assertEqual(len(self.service.data_lake), 0)
        self.service.data_sources = original_sources # Restore

# tests/test_market_data_processing.py
import unittest
from datetime import datetime
from src.services.market_data_processing import MarketDataProcessingService
from src.models.report_models import RawMarketData, ProcessedMarketData
from src.utils.llm_utils import LLMService

class TestMarketDataProcessingService(unittest.TestCase):
    def setUp(self):
        self.llm_service = LLMService() # Use the simulated LLMService
        self.service = MarketDataProcessingService(llm_service=self.llm_service)
        # Clear in-memory data warehouse
        self.service.data_warehouse = []

        self.mock_raw_data = [
            RawMarketData(
                source="news_api",
                timestamp=datetime.now().isoformat(),
                content={"title": "Test News 1", "text": "This is a test article about TechCorp's new AI product."},
                data_id="raw_123"
            ),
            RawMarketData(
                source="sec_filings_db",
                timestamp=datetime.now().isoformat(),
                content={"company": "BioLabs", "filing_type": "10-K", "content_summary": "Financial report highlighting growth in biotech."},
                data_id="raw_456"
            )
        ]

    def test_process_raw_data_success(self):
        """Test successful processing of raw data."""
        processed_ids = self.service.process_raw_data(self.mock_raw_data)
        self.assertEqual(len(processed_ids), len(self.mock_raw_data))
        self.assertEqual(len(self.service.data_warehouse), len(self.mock_raw_data))

        # Verify structure and content of processed data
        for processed_id in processed_ids:
            processed_data = next((d for d in self.service.data_warehouse if d.original_id == processed_id), None)
            self.assertIsNotNone(processed_data)
            self.assertIsInstance(processed_data, ProcessedMarketData)
            self.assertNotEqual(processed_data.summary, "No summary extracted.") # Should have an LLM summary
            self.assertGreater(len(processed_data.companies), 0) # Should extract some companies
            self.assertIsNotNone(processed_data.processed_at)

    def test_get_processed_data(self):
        """Test retrieving specific processed data entries."""
        processed_ids = self.service.process_raw_data(self.mock_raw_data)
        retrieved_data = self.service.get_processed_data([processed_ids[0]])
        self.assertEqual(len(retrieved_data), 1)
        self.assertEqual(retrieved_data[0].original_id, processed_ids[0])

    def test_process_empty_list(self):
        """Test processing an empty list of raw data."""
        processed_ids = self.service.process_raw_data([])
        self.assertEqual(len(processed_ids), 0)
        self.assertEqual(len(self.service.data_warehouse), 0)


# tests/test_llm_inference.py
import unittest
from datetime import datetime
from src.services.llm_inference import LLMInferenceService
from src.models.report_models import ProcessedMarketData, LLMInsight
from src.utils.llm_utils import LLMService

class TestLLMInferenceService(unittest.TestCase):
    def setUp(self):
        self.llm_service = LLMService() # Use the simulated LLMService
        self.service = LLMInferenceService(llm_service=self.llm_service)
        self.service.insights_store = [] # Clear insights store

        self.mock_processed_data = [
            ProcessedMarketData(
                original_id="proc_1",
                industry_sector="Tech",
                companies=["TechCorp"],
                keywords=["AI", "Cloud"],
                summary="TechCorp reports strong growth in AI and cloud services.",
                processed_at=datetime.now().isoformat(),
                structured_data={}
            ),
            ProcessedMarketData(
                original_id="proc_2",
                industry_sector="Energy",
                companies=[],
                keywords=["Solar", "Investment"],
                summary="Significant new investments in solar energy projects.",
                processed_at=datetime.now().isoformat(),
                structured_data={}
            )
        ]

    def test_generate_industry_analysis(self):
        """Test generation of industry analysis."""
        analysis = self.service.generate_industry_analysis(self.mock_processed_data)
        self.assertIn("overview", analysis)
        self.assertIsInstance(analysis["overview"], str)
        self.assertGreater(len(analysis["overview"]), 50) # Ensure content is generated

    def test_generate_competitive_landscape(self):
        """Test generation of competitive landscape analysis."""
        landscape = self.service.generate_competitive_landscape(self.mock_processed_data, ["TechCorp"])
        self.assertIn("overview", landscape)
        self.assertIsInstance(landscape["overview"], str)
        self.assertIn("TechCorp", landscape["key_players_identified"])
        self.assertIn("swot_analysis", landscape)

    def test_identify_market_trends_and_predictions(self):
        """Test identification of market trends and predictions."""
        trends = self.service.identify_market_trends_and_predictions(self.mock_processed_data)
        self.assertIn("identified_trends", trends)
        self.assertGreater(len(trends["identified_trends"]), 0)
        self.assertIn("future_predictions", trends)
        self.assertIsInstance(trends["future_predictions"], str)

    def test_analyze_technology_adoption(self):
        """Test analysis of technology adoption."""
        adoption = self.service.analyze_technology_adoption(self.mock_processed_data)
        self.assertIn("overview", adoption)
        self.assertIsInstance(adoption["overview"], str)
        self.assertIn("AI", adoption["technology_focus"])

    def test_generate_strategic_insights(self):
        """Test generation of strategic insights."""
        # Mock outputs from other services for insight generation
        mock_industry_analysis = {"overview": "Industry growing due to AI."}
        mock_competitive_landscape = {"overview": "New players disrupting."}
        mock_market_trends = {"future_predictions": "Sustainability is key trend."}
        mock_tech_adoption = {"overview": "AI adoption is high."}

        insights = self.service.generate_strategic_insights(
            mock_industry_analysis, mock_competitive_landscape, mock_market_trends, mock_tech_adoption
        )
        self.assertGreater(len(insights), 0)
        self.assertIsInstance(insights[0], LLMInsight)
        self.assertIn("description", insights[0].dict())
        self.assertIn("insight_type", insights[0].dict())


# tests/test_analytics_insights.py
import unittest
from datetime import datetime
from src.services.analytics_insights import AnalyticsInsightsService
from src.models.report_models import LLMInsight, Recommendation, ProcessedMarketData
from src.utils.llm_utils import LLMService

class TestAnalyticsInsightsService(unittest.TestCase):
    def setUp(self):
        self.llm_service = LLMService()
        self.service = AnalyticsInsightsService(llm_service=self.llm_service)

        self.mock_llm_insights = [
            LLMInsight(
                insight_type="Market Opportunity",
                description="Surging demand for sustainable products.",
                relevance_score=0.9,
                confidence_score=0.85,
                supporting_data_ids=[],
                generated_at=datetime.now().isoformat()
            ),
            LLMInsight(
                insight_type="Competitive Threat",
                description="New AI startups are disrupting the market.",
                relevance_score=0.8,
                confidence_score=0.7,
                supporting_data_ids=[],
                generated_at=datetime.now().isoformat()
            )
        ]
        self.mock_processed_data = [
            ProcessedMarketData(
                original_id="proc_1",
                industry_sector="General",
                companies=[], keywords=[],
                summary="Consumers are increasingly opting for sustainable and eco-friendly goods.",
                processed_at=datetime.now().isoformat(),
                structured_data={}
            ),
            ProcessedMarketData(
                original_id="proc_2",
                industry_sector="Tech",
                companies=[], keywords=[],
                summary="Several new AI-powered companies recently launched products targeting the smart home sector.",
                processed_at=datetime.now().isoformat(),
                structured_data={}
            )
        ]
        self.mock_analysis_context = {
            "industry_name": "Consumer Goods",
            "market_trends": ["Sustainability", "Digitalization"],
            "tech_focus": ["AI"]
        }

    def test_validate_llm_insights(self):
        """Test validation of LLM insights."""
        validated_insights = self.service.validate_llm_insights(self.mock_llm_insights, self.mock_processed_data)
        # Both insights should pass with the current simple validation logic
        self.assertEqual(len(validated_insights), 2)
        self.assertEqual(validated_insights[0].insight_type, "Market Opportunity")
        self.assertEqual(validated_insights[1].insight_type, "Competitive Threat")

        # Test with a low-confidence insight that should be filtered
        low_confidence_insight = LLMInsight(
            insight_type="Speculative",
            description="Aliens will buy all the stocks.",
            relevance_score=0.1,
            confidence_score=0.5, # Below threshold
            supporting_data_ids=[],
            generated_at=datetime.now().isoformat()
        )
        validated_with_low_confidence = self.service.validate_llm_insights(self.mock_llm_insights + [low_confidence_insight], self.mock_processed_data)
        self.assertEqual(len(validated_with_low_confidence), 2) # Low confidence insight should be filtered out

    def test_generate_actionable_recommendations(self):
        """Test generation of actionable recommendations."""
        recommendations = self.service.generate_actionable_recommendations(
            self.mock_llm_insights, self.mock_analysis_context
        )
        self.assertGreater(len(recommendations), 0)
        self.assertIsInstance(recommendations[0], Recommendation)
        self.assertIn("action_steps", recommendations[0].dict())
        self.assertIn("expected_impact", recommendations[0].dict())
        self.assertIn("priority", recommendations[0].dict())


# tests/test_report_generation.py
import unittest
import os
import shutil
from datetime import datetime
from src.services.report_generation import ReportGenerationService
from src.models.report_models import (
    ReportRequest, LLMInsight, Recommendation, ExecutiveSummary, ReportContent
)
from src.utils.llm_utils import LLMService
from src.config import Config

class TestReportGenerationService(unittest.TestCase):
    def setUp(self):
        self.llm_service = LLMService()
        self.service = ReportGenerationService(llm_service=self.llm_service)
        # Create a temporary directory for reports
        self.test_reports_dir = "test_reports"
        Config.REPORTS_PATH = self.test_reports_dir # Override config for testing
        os.makedirs(self.test_reports_dir, exist_ok=True)

        self.mock_request_id = "test_report_123"
        self.mock_report_request = ReportRequest(
            request_id=self.mock_request_id,
            industry="Test Industry",
            focus_areas=["Test Area"],
            report_format="md"
        )
        self.mock_industry_analysis = {"overview": "Test industry is stable."}
        self.mock_competitive_landscape = {"overview": "Key player A dominates."}
        self.mock_market_trends_predictions = {"future_predictions": "Growth is expected."}
        self.mock_technology_adoption_analysis = {"overview": "Tech X is widely adopted."}
        self.mock_strategic_insights = [
            LLMInsight(
                insight_type="Opportunity",
                description="New market opening.",
                relevance_score=0.9, confidence_score=0.9,
                supporting_data_ids=[], generated_at=datetime.now().isoformat()
            )
        ]
        self.mock_actionable_recommendations = [
            Recommendation(
                category="Strategy",
                description="Expand into new regions.",
                action_steps=["Step 1", "Step 2"],
                expected_impact="Increased revenue.",
                priority="High",
                related_insights=[]
            )
        ]

    def tearDown(self):
        # Clean up the temporary directory
        if os.path.exists(self.test_reports_dir):
            shutil.rmtree(self.test_reports_dir)
        # Restore original config path
        Config.REPORTS_PATH = "reports/"

    def test_generate_executive_summary_llm(self):
        """Test LLM-based executive summary generation."""
        mock_report_content = ReportContent(
            executive_summary=ExecutiveSummary(key_findings=[], strategic_implications=[], top_recommendations=[], summary_text=""),
            industry_analysis=self.mock_industry_analysis,
            competitive_landscape=self.mock_competitive_landscape,
            market_trends_predictions=self.mock_market_trends_predictions,
            technology_adoption_analysis=self.mock_technology_adoption_analysis,
            strategic_insights=self.mock_strategic_insights,
            actionable_recommendations=self.mock_actionable_recommendations
        )
        summary = self.service._generate_executive_summary_llm(mock_report_content)
        self.assertIsInstance(summary, ExecutiveSummary)
        self.assertGreater(len(summary.summary_text), 50) # Ensure content is generated
        self.assertGreater(len(summary.key_findings), 0)
        self.assertGreater(len(summary.top_recommendations), 0)

    def test_generate_report(self):
        """Test full report generation to file."""
        report_path = self.service.generate_report(
            self.mock_request_id,
            self.mock_industry_analysis,
            self.mock_competitive_landscape,
            self.mock_market_trends_predictions,
            self.mock_technology_adoption_analysis,
            self.mock_strategic_insights,
            self.mock_actionable_recommendations,
            self.mock_report_request
        )
        self.assertTrue(os.path.exists(report_path))
        self.assertIn(self.mock_request_id, report_path)
        self.assertTrue(report_path.endswith(".md"))

        # Verify content by reading the file (basic check)
        with open(report_path, "r") as f:
            content = f.read()
            self.assertIn("Executive Summary", content)
            self.assertIn("Industry Analysis", content)
            self.assertIn(self.mock_strategic_insights[0].description, content)
            self.assertIn(self.mock_actionable_recommendations[0].description, content)


# tests/test_main.py
import unittest
import os
import shutil
from unittest.mock import MagicMock, patch
from src.main import MarketResearchFramework
from src.models.report_models import ReportRequest, ReportStatus
from src.config import Config

class TestMarketResearchFramework(unittest.TestCase):
    def setUp(self):
        # Patch external services/dependencies to isolate Main framework logic
        # For full integration testing, these would not be mocked.
        self.patcher_llm_service = patch('src.main.LLMService')
        self.mock_llm_service_cls = self.patcher_llm_service.start()
        self.mock_llm_service_instance = MagicMock()
        self.mock_llm_service_cls.return_value = self.mock_llm_service_instance
        self.mock_llm_service_instance.generate_response.return_value = "Simulated LLM Response."
        self.mock_llm_service_instance.extract_entities_and_summary.return_value = {"summary": "simulated summary", "entities": ["simulated_entity"]}

        self.patcher_data_ingestion = patch('src.main.DataIngestionService')
        self.mock_data_ingestion_service_cls = self.patcher_data_ingestion.start()
        self.mock_data_ingestion_service_instance = MagicMock()
        self.mock_data_ingestion_service_cls.return_value = self.mock_data_ingestion_service_instance
        self.mock_data_ingestion_service_instance.ingest_data.return_value = ["raw_id_1", "raw_id_2"]
        self.mock_data_ingestion_service_instance.get_raw_data.return_value = [
            MagicMock(data_id="raw_id_1", content={"text": "raw content 1"}),
            MagicMock(data_id="raw_id_2", content={"text": "raw content 2"})
        ]

        self.patcher_market_data_processing = patch('src.main.MarketDataProcessingService')
        self.mock_market_data_processing_service_cls = self.patcher_market_data_processing.start()
        self.mock_market_data_processing_service_instance = MagicMock()
        self.mock_market_data_processing_service_cls.return_value = self.mock_market_data_processing_service_instance
        self.mock_market_data_processing_service_instance.process_raw_data.return_value = ["proc_id_1", "proc_id_2"]
        self.mock_market_data_processing_service_instance.get_processed_data.return_value = [
            MagicMock(original_id="proc_id_1", summary="proc content 1", companies=["CompanyA"]),
            MagicMock(original_id="proc_id_2", summary="proc content 2", companies=["CompanyB"])
        ]

        self.patcher_llm_inference = patch('src.main.LLMInferenceService')
        self.mock_llm_inference_service_cls = self.patcher_llm_inference.start()
        self.mock_llm_inference_service_instance = MagicMock()
        self.mock_llm_inference_service_cls.return_value = self.mock_llm_inference_service_instance
        self.mock_llm_inference_service_instance.generate_industry_analysis.return_value = {"overview": "mock industry analysis"}
        self.mock_llm_inference_service_instance.generate_competitive_landscape.return_value = {"overview": "mock competitive landscape"}
        self.mock_llm_inference_service_instance.identify_market_trends_and_predictions.return_value = {"future_predictions": "mock trends"}
        self.mock_llm_inference_service_instance.analyze_technology_adoption.return_value = {"overview": "mock tech adoption"}
        self.mock_llm_inference_service_instance.generate_strategic_insights.return_value = [
            MagicMock(insight_type="Opportunity", description="Mock insight.", relevance_score=0.9, confidence_score=0.8, supporting_data_ids=[], generated_at="now")
        ]

        self.patcher_analytics_insights = patch('src.main.AnalyticsInsightsService')
        self.mock_analytics_insights_service_cls = self.patcher_analytics_insights.start()
        self.mock_analytics_insights_service_instance = MagicMock()
        self.mock_analytics_insights_service_cls.return_value = self.mock_analytics_insights_service_instance
        self.mock_analytics_insights_service_instance.validate_llm_insights.return_value = [
            MagicMock(insight_type="Opportunity", description="Validated mock insight.", relevance_score=0.9, confidence_score=0.8, supporting_data_ids=[], generated_at="now")
        ]
        self.mock_analytics_insights_service_instance.generate_actionable_recommendations.return_value = [
            MagicMock(category="General", description="Mock recommendation.", action_steps=["step"], expected_impact="impact", priority="High", related_insights=[])
        ]

        self.patcher_report_generation = patch('src.main.ReportGenerationService')
        self.mock_report_generation_service_cls = self.patcher_report_generation.start()
        self.mock_report_generation_service_instance = MagicMock()
        self.mock_report_generation_service_cls.return_value = self.mock_report_generation_service_instance
        self.mock_report_generation_service_instance.generate_report.return_value = "/mock/path/to/report.md"

        self.patcher_continuous_monitoring = patch('src.main.ContinuousMonitoringService')
        self.mock_continuous_monitoring_service_cls = self.patcher_continuous_monitoring.start()
        self.mock_continuous_monitoring_service_instance = MagicMock()
        self.mock_continuous_monitoring_service_cls.return_value = self.mock_continuous_monitoring_service_instance

        # Temporary directory for reports for actual file system interaction in ReportGenerationService
        self.test_reports_dir = "test_reports_main"
        Config.REPORTS_PATH = self.test_reports_dir # Override config for testing
        os.makedirs(self.test_reports_dir, exist_ok=True)

        self.framework = MarketResearchFramework()

    def tearDown(self):
        self.patcher_llm_service.stop()
        self.patcher_data_ingestion.stop()
        self.patcher_market_data_processing.stop()
        self.patcher_llm_inference.stop()
        self.patcher_analytics_insights.stop()
        self.patcher_report_generation.stop()
        self.patcher_continuous_monitoring.stop()

        if os.path.exists(self.test_reports_dir):
            shutil.rmtree(self.test_reports_dir)
        Config.REPORTS_PATH = "reports/" # Restore original config

    def test_generate_market_research_report_success(self):
        """Test the end-to-end report generation process for success."""
        request = ReportRequest(
            request_id="test_req_success",
            industry="Test Industry",
            focus_areas=["Focus 1"],
            report_format="md"
        )
        status = self.framework.generate_market_research_report(request)

        self.assertEqual(status.status, "COMPLETED")
        self.assertEqual(status.progress, 100.0)
        self.assertIsNotNone(status.report_path)
        self.mock_data_ingestion_service_instance.ingest_data.assert_called_once()
        self.mock_market_data_processing_service_instance.process_raw_data.assert_called_once()
        self.mock_llm_inference_service_instance.generate_strategic_insights.assert_called_once()
        self.mock_analytics_insights_service_instance.generate_actionable_recommendations.assert_called_once()
        self.mock_report_generation_service_instance.generate_report.assert_called_once()

    def test_generate_market_research_report_failure_no_data(self):
        """Test report generation failure when no raw data is ingested."""
        self.mock_data_ingestion_service_instance.ingest_data.return_value = [] # Simulate no data ingested

        request = ReportRequest(
            request_id="test_req_no_data",
            industry="Test Industry",
            focus_areas=["Focus 1"],
            report_format="md"
        )
        status = self.framework.generate_market_research_report(request)

        self.assertEqual(status.status, "FAILED")
        self.assertEqual(status.progress, 0.0)
        self.assertIn("No raw data ingested", status.error_message)

    def test_get_report_status(self):
        """Test retrieving report status."""
        request = ReportRequest(
            request_id="test_req_status",
            industry="Test Industry",
            focus_areas=["Focus 1"],
            report_format="md"
        )
        # Generate a report first to have a status
        self.framework.generate_market_research_report(request)

        retrieved_status = self.framework.get_report_status(request.request_id)
        self.assertEqual(retrieved_status.request_id, request.request_id)
        self.assertEqual(retrieved_status.status, "COMPLETED")

        # Test for a non-existent report
        non_existent_status = self.framework.get_report_status("non_existent_id")
        self.assertEqual(non_existent_status.status, "NOT_FOUND")

    def test_start_continuous_monitoring(self):
        """Test that continuous monitoring loop is started."""
        # Ensure that `start_monitoring_loop` is called on the mock service
        self.framework.start_continuous_monitoring(duration_seconds=5)
        self.mock_continuous_monitoring_service_instance.start_monitoring_loop.assert_called_once_with(duration_seconds=5)

```

### Installation and Usage Instructions

To set up and use this LLM-guided Gartner-style market research report generating framework, follow these steps:

#### Prerequisites

*   Python 3.9+
*   `pip` (Python package installer)

#### 1. Clone the Repository (Conceptual)

In a real scenario, you would clone the project from a Git repository:

```bash
git clone https://github.com/your_org/market-research-framework.git
cd market-research-framework
```

#### 2. Create and Activate a Virtual Environment

It's highly recommended to use a virtual environment to manage dependencies:

```bash
python -m venv .venv
# On macOS/Linux:
source .venv/bin/activate
# On Windows:
.venv\Scripts\activate
```

#### 3. Install Dependencies

You would typically have a `requirements.txt` file. For this project, the primary dependency is `pydantic`. If integrating with real LLMs, you'd add their SDKs (e.g., `openai`, `google-generativeai`).

```bash
pip install pydantic
# If using OpenAI: pip install openai
# If using Google Generative AI: pip install google-generativeai
```

#### 4. Configure Environment Variables

Create a `.env` file in the project root or set environment variables directly. This is crucial for LLM API keys and other sensitive configurations.

```bash
# .env file content (example)
LLM_API_KEY="YOUR_ACTUAL_LLM_API_KEY_HERE"
LLM_MODEL_NAME="gpt-4o" # Or "gemini-pro", "claude-3-opus-20240229", etc.
LOG_LEVEL="INFO"
```

**Note**: For production deployments, use a secure secrets management system (e.g., AWS Secrets Manager, HashiCorp Vault) instead of `.env` files.

#### 5. Running the Framework

The `main.py` script demonstrates how to initiate report generation.

```bash
python src/main.py
```

This will:
*   Initialize the framework and its services.
*   Generate two sample reports (`market_research_report_*.md` in the `reports/` directory).
*   Demonstrate the continuous monitoring feature by re-triggering one of the reports.

#### 6. Viewing Generated Reports

The reports are generated in markdown (`.md`) format for easy viewing. You can open them with any text editor or markdown viewer. The console output will provide the exact file paths.

#### 7. Running Unit Tests

To ensure the integrity of the codebase, run the unit tests:

```bash
python -m unittest discover tests
```

This command will discover and execute all test files within the `tests/` directory.

#### 8. Extending the Framework

*   **Add New Data Sources**: Implement new `_fetch_from_source` methods or create new connector modules in `src/services/data_ingestion.py`.
*   **Integrate Real LLMs**: Replace the simulated LLM calls in `src/utils/llm_utils.py` with actual API calls using the respective LLM SDKs (e.g., `openai` for OpenAI, `google-generativeai` for Gemini).
*   **Enhance Data Processing**: Add more sophisticated data cleaning, normalization, and feature engineering steps in `src/services/market_data_processing.py`.
*   **Refine Prompt Engineering**: Experiment with different prompts in `src/services/llm_inference.py` and `src/services/report_generation.py` to get better LLM outputs.
*   **Improve Report Formatting**: Extend `src/services/report_generation.py` to support other output formats (PDF, DOCX) using libraries like `ReportLab` for PDF or `python-docx` for Word documents.
*   **Implement Message Broker**: For a true microservices architecture, integrate a message broker (e.g., Kafka, RabbitMQ) to facilitate asynchronous communication between services. You'd replace the direct method calls with event publishing/consuming.
*   **Build a UI**: Develop a separate frontend application (e.g., using React, Angular, Vue.js) that interacts with an API Gateway exposing the framework's functionalities (e.g., FastAPI endpoints).

This framework provides a solid foundation for building a robust LLM-guided market research report generation system.

---
*Saved by after_agent_callback on 2025-07-06 14:40:59*
